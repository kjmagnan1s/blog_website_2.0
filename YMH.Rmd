---
title: "Your Mom's House analysis"
author: "Kevin Magnan"
date: "10.04.2020"
output: html_document
---

Inspired by a fellow YMH podcast (warning NSFW) fan/mommy [Eric](https://eric-ekholm.netlify.app/), I am trying my hand at analyzing YMH podcast transcripts. I plan on following along with Eric's work flow below and diverting once I find an opportunity to personalize the analysis - aka keep it high and tight.

Eric's blog post analysis involved pulling transcripts from YMH YouTube videos. I don't have any experience in pulling transcripts from youtube nor any serious text analysis so I'm excited to try this out and learn from a Eric. Thanks for not being stingy, Mark.... I mean Eric!

# Libraries

```{r global-options, include=TRUE}
knitr::opts_chunk$set(echo = FALSE)

library(tidyverse)
library(youtubecaption)
library(janitor)
library(lubridate)
library(reshape2)
```

Aside from {tidyverse}, {reshape2}, and {lubridate}, Eric uses {youtubecaption} for calling YouTube's API to get the caption transcripts and {janitor} for some of the text analysis, namely the clean_names() function. These two packages are brand new to me so I'm excited to work with them.

Note: I learned further into this analysis that {youtubecaption} relies on Anaconda, a comprehensive Python environment, to pull from YouTube's API. The author of youtubecaption states the reason for this due to the difficulty of calling YouTube's API for the captain data in a user friendly format. Thus, this library acts as a handy and clean way of accessing YouTube's API in Python (Anaconda) without needing to write a custom API call in another language. I didn't have Anaconda on this machine so it was not able to run and I had to take a break from the analysis to install Anaconda. Save yourself some time and [download it](https://www.anaconda.com/download/) before getting started!

# Single YouTube API call, episode 571

```{r}
ymh_e571 <- get_caption("https://www.youtube.com/watch?v=dY818rfsJkk")
```

And there we go, really straight forward way to download a transcript of a YouTube video. Let's take a look at what we are working with:

```{r}
head(ymh_e571)
```

Upon inspection, the YouTube API call gives us 5 columns and just over 4500 rows or data. 'Segment_id' would be our unique identifier for each segment, 'text' is obviously the speech-to-text captions we are after, 'start' appears to be the time stamp when the text is recorded, the value of 'duration' is a mystery at the moment but we'll look into that soon, and 'vid' is simply the YouTube html address to the video.

The next step would be to download the transcripts for a number of episodes, but so far we've only made an API call to YouTube using the exact URL for a video. How are we going to call a list of videos or even all the videos outside of manually looking up all 571 YMH videos? Eric figured out a nice workaround on his blog and I'm going to use it here.

# Create YMH URL list

What we can do to script out a large API call is take a YMH YouTube playlist and export that to a text file. From there, we have a full list of YMH videos, their names, and, most importantly, the URLs. ~~There are plenty of web tools and extensions to export a YouTUbe data to a .csv and only require a quick Google search to find one. Eric used [this tool](http://www.williamsportwebdeveloper.com/FavBackUp.aspx) which looks straight forward and conveinent. Once you have the list, we'll need to clean and arrange the file to use when calling from the YouTube API.~~

Houston, we have a problem:

<img src="https://raw.githubusercontent.com/kjmagnan1s/kjmagnan1s.github.io/master/images/web_yt_error.png"
width= 100% height= 100%/>

Turns out the web app Eric used to gather the YouTube playlist and episode information is no longer working.  We will have to find other means of generating this list to make the API call. After a painstaking Sunday searching for alternative web apps and testing them, I was not able to find anything that fit the needs for this project. That is until I stumbled upon [YouTube-dl](https://github.com/ytdl-org/youtube-dl), a command-line program created to download YouTube videos and/or audio tracks from YouTube and a host of other sites. It took some time (and reddit/r/youtube-dl threads) to figure out how to use the tool to generate a list of playlist video titles and URLs, but I come to you with the knowledge! So lets do this, shall we?

First thing you'll want to do is download the youtube-dl .exe windows application from their site and place it into a folder. Open a cmd window and navigate to that folder directory. From here I've done all the hard work of figuring out the perfect syntax below:

```
youtube-dl --get-filename --get-title -i -o "https://www.youtube.com/watch?v=%(id)s" "https://www.youtube.com/playlist?list=PL-i3EV1v5hLd9H1p2wT5ZD8alEY0EmxYD" > ymhplaylist.csv
```

What you should end up with is a csv of episode titles and their accompanying URL. Let's take a look:

```{r}
playlist_raw <- read.csv("~/GitHub/kjmagnan1s.github.io/blog_data/ymhplaylist.csv", header=FALSE)

head(playlist_raw)
```

You didn't think it would be that easy, did you? Turns out the command-line export creates a long data set and not a wide data set so we need to fix that. It also appears to be the case that youtube-dl's URL output format injected some hashtags '#' in place of the actual URL character so we'll have to clean that as well.  

# Data Cleaning

 We'll do some reshaping and data cleaning below. I'm not the best equipped to explain these data cleaning steps and I know there are probably simpler, faster, and more efficient ways of doing it. I'd suggest searching for some of the experts on {tidyverse} and {reshape2} to learn about it!
 
```{r}
# filtering out the episode titles and episode urls into separate columns
playlist_url <- playlist_raw %>% filter(grepl("https", V1)) %>% rename("url" = V1)
playlist_title <- playlist_raw %>% filter(grepl("Your", V1)) %>% rename("title" = V1)
playlist_clean <- cbind(playlist_title, playlist_url)
head(playlist_clean)
# replacing the strange '#' bugged output from youtube-dl
playlist_clean$url <- gsub("#\\", ":\\", playlist_clean$url)
playlist_clean$url <- gsub("\\\\", "\\", playlist_clean$url)
playlist_clean$url <- gsub("#v", "?v", playlist_clean$url)
playlist_clean <- playlist_clean %>% 
  clean_names() %>%                                        # systematically cleans any text or symbols up 
  separate(title, c("title", "episode"), "-") %>%          # separate episode title by '-'
  mutate(vid = str_replace_all(url, ".*=(.*)$", "\\1"))    # creates a vid column of just the youtube short link, this column matches the "vid" output from get_caption() and will be used to join the two tables
head(playlist_clean)
```

Success! We have 240 rows of observations of YMH episodes, #330 to #571, with titles and URLs. Now we can start working on pulling the transcripts for all 240 episodes.

# List API call

The next steps is to use the get_caption() function again with the {youtubecaption} package.We need to loop the get_caption() function so we can pull all the captions from the list of urls but we also need to make sure the get_caption() function runs successfully through the loop and alerts us to any video it cannot successfully call. The way Eric did this was to wrap the get_caption() function inside the safely() function from {purrr}. 

```{r}
?safely                       
safe_cap <- safely(get_caption)
```

Based on the help documents, safely() provides an enhanced output of errors, messages, and warnings outside of the base r reporting.  This function should help determine if our large API call has an errors while running.

Within the same {purrr} library, the map() function applies a function to each element of a list. Therefore, in out case, map() will apply the get_caption() function to the URL list and wrapping safely() around get_caption() provided added outputs and errors.

```{r}
playlist_call <- map(playlist_clean$url,
                     safe_cap)
head(playlist_call)
```

# Finalizing the API call data

With the list API call finished, the return of the map() function resulted in a vector of the 240 captions. But instead of a single output, we have a result output and an error output from the safely() function and we want to get the 'result' outputs from each vector.

To do this, we will again call the map() function as well as the pluck() function from {purrr} (you got to love these function names from a library called {purrr} :D). Pluck() essentially indexes a data structure and returns the desired object, in our case the result caption text from each URL. 

```{r}
playlist_data <- map(1:length(playlist_call),
                     ~pluck(playlist_call, ., "result") %>%
                       compact() %>%
                       bind_rows() %>%
                       inner_join(x = playlist_call,
                                  y = .,
                                  by = "vid"))

head(playlist_data)
```



